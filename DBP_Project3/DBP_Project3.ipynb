{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rEDBCGvP2i-"
      },
      "source": [
        "# Database Project (SWE3033) (Fall 2024)\n",
        "# Project 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuhrAzOWP2i_"
      },
      "source": [
        "**Instruction:** In this homework, we provide you with a jupyter notebook file (DBP_Project3.ipynb). You should follow the instructions in these documents carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fna905IJP2jA"
      },
      "source": [
        "**Submit two files as follows**:\n",
        "- `DBP_Project3_StudentID.zip`\n",
        "\t- `DBP_Project3_StudentID.ipynb`\n",
        "\t- `DBP_Project3_StudentID.pdf`\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UEmjYk4P2jA",
        "outputId": "ac4b1203-f4fc-4ba7-fec7-be4e1b9e8fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVMh0rHQP2jB"
      },
      "source": [
        "### Import required library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LW3wnAGaP2jC"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFdbJsWP2jC"
      },
      "source": [
        "# 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmeoGNmP2jD"
      },
      "source": [
        "#### (a) Create a DataFrame with the given data and display the generated DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-PgnB_dP2jD",
        "outputId": "5cfb5589-6456-48cb-f956-acc1c1f016ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---------+\n",
            "|doc_id|topic |timestamp|\n",
            "+------+------+---------+\n",
            "|1     |Mac   |10009820 |\n",
            "|2     |iPhone|10009830 |\n",
            "|3     |iPhone|10009900 |\n",
            "|4     |Galaxy|10009950 |\n",
            "|5     |iPhone|10010000 |\n",
            "|6     |A100  |10010010 |\n",
            "|7     |Galaxy|10010030 |\n",
            "|8     |iPhone|10010050 |\n",
            "|9     |A100  |10010070 |\n",
            "+------+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(doc_id='1', topic='Mac', timestamp='10009820'),\n",
        "    Row(doc_id='2', topic='iPhone', timestamp='10009830'),\n",
        "    Row(doc_id='3', topic='iPhone', timestamp='10009900'),\n",
        "    Row(doc_id='4', topic='Galaxy', timestamp='10009950'),\n",
        "    Row(doc_id='5', topic='iPhone', timestamp='10010000'),\n",
        "    Row(doc_id='6', topic='A100', timestamp='10010010'),\n",
        "    Row(doc_id='7', topic='Galaxy', timestamp='10010030'),\n",
        "    Row(doc_id='8', topic='iPhone', timestamp='10010050'),\n",
        "    Row(doc_id='9', topic='A100', timestamp='10010070')\n",
        "]\n",
        "\n",
        ")\n",
        "df = RDD.toDF()\n",
        "\n",
        "# =====================================\n",
        "\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4gHr4e8P2jE"
      },
      "source": [
        "#### (b) After adding the two additional documents, find the timestamp for each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L3Pc3-oP2jE",
        "outputId": "5e5bcb0b-37fc-453d-accc-62b32f9fc505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---------+\n",
            "|doc_id|topic |timestamp|\n",
            "+------+------+---------+\n",
            "|1     |Mac   |10009820 |\n",
            "|2     |iPhone|10009830 |\n",
            "|3     |iPhone|10009900 |\n",
            "|4     |Galaxy|10009950 |\n",
            "|5     |iPhone|10010000 |\n",
            "|6     |A100  |10010010 |\n",
            "|7     |Galaxy|10010030 |\n",
            "|8     |iPhone|10010050 |\n",
            "|9     |A100  |10010070 |\n",
            "|10    |A6000 |10010100 |\n",
            "|11    |H100  |10010500 |\n",
            "+------+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "additional_doc = sc.parallelize([\n",
        "    Row(doc_id='10', topic='A6000', timestamp='10010100'),\n",
        "    Row(doc_id='11', topic='H100', timestamp='10010500')]\n",
        ")\n",
        "new_df = df.union(additional_doc.toDF())\n",
        "\n",
        "# =====================================\n",
        "new_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddRBRAKxP2jF"
      },
      "source": [
        "#### (c) Group the data in the joined DataFrame by ‘topic’ column and count the number of data for each topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWjuH3TqP2jF",
        "outputId": "0bbfad71-2f41-45b2-e112-b67d8c017485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|topic |count|\n",
            "+------+-----+\n",
            "|iPhone|4    |\n",
            "|Galaxy|2    |\n",
            "|Mac   |1    |\n",
            "|A100  |2    |\n",
            "|A6000 |1    |\n",
            "|H100  |1    |\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "group_df = new_df.groupBy('topic').count()\n",
        "\n",
        "# =====================================\n",
        "group_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdtjmxX3P2jF"
      },
      "source": [
        "## 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA57GAzKP2jF"
      },
      "source": [
        "#### (a) Create a DataFrame for the two given datasets and join Data 1 with Data 2 using an inner join based on the ‘topic’ and ‘timestamp’ column. (left side: Data 2, right side: Data 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PJhOiQoP2jG",
        "outputId": "b7f12c84-3201-4fdc-da27-b12630906440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------------+------+------+---------+\n",
            "|topic_join|view|timestamp_join|doc_id| topic|timestamp|\n",
            "+----------+----+--------------+------+------+---------+\n",
            "|      A100|3000|      10010070|     9|  A100| 10010070|\n",
            "|     A6000|2000|      10010100|    10| A6000| 10010100|\n",
            "|    Galaxy| 200|      10009950|     4|Galaxy| 10009950|\n",
            "|      H100|9000|      10010500|    11|  H100| 10010500|\n",
            "|       Mac|1000|      10009820|     1|   Mac| 10009820|\n",
            "|    iPhone| 400|      10009900|     3|iPhone| 10009900|\n",
            "+----------+----+--------------+------+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(topic='Mac', view=1000, timestamp=10009820),\n",
        "    Row(topic='Galaxy', view=200, timestamp=10009950),\n",
        "    Row(topic='iPhone', view=400, timestamp=10009900),\n",
        "    Row(topic='A100', view=3000, timestamp=10010070),\n",
        "    Row(topic='A6000', view=2000, timestamp=10010100),\n",
        "    Row(topic='H100', view=9000, timestamp=10010500)\n",
        "\n",
        "])\n",
        "DF_join = RDD.toDF()\n",
        "DF_join = DF_join.withColumnRenamed(\"topic\", \"topic_join\")\n",
        "DF_join = DF_join.withColumnRenamed(\"timestamp\", \"timestamp_join\")\n",
        "\n",
        "DF_join = DF_join.join(new_df, (DF_join.topic_join == new_df.topic) & (DF_join.timestamp_join == new_df.timestamp), \"inner\")\n",
        "\n",
        "# =====================================\n",
        "DF_join.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99OtXhSRP2jG"
      },
      "source": [
        "#### (b) Convert the data type of the ‘doc_id’ and ‘timestamp’ columns to Integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIVpp1LMP2jG",
        "outputId": "ba5efea5-f4c7-45c2-dbfa-9a8f555e006f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- topic_join: string (nullable = true)\n",
            " |-- view: long (nullable = true)\n",
            " |-- timestamp_join: long (nullable = true)\n",
            " |-- doc_id: integer (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "DF_join = DF_join.withColumn(\"doc_id\", DF_join[\"doc_id\"].cast(\"integer\"))\n",
        "DF_join = DF_join.withColumn(\"timestamp\", DF_join[\"timestamp\"].cast(\"integer\"))\n",
        "\n",
        "\n",
        "# =====================================\n",
        "DF_join.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHKRDK5eP2jG"
      },
      "source": [
        "#### (c) Use an SQL query to select the data from joined DataFrame where the ‘view’ is greater than 1500. And briefly explain the method you used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wD4J9DcP2jH",
        "outputId": "202c3753-5cee-4786-a536-5fffb7ad0163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------------+------+-----+---------+\n",
            "|topic_join|view|timestamp_join|doc_id|topic|timestamp|\n",
            "+----------+----+--------------+------+-----+---------+\n",
            "|      A100|3000|      10010070|     9| A100| 10010070|\n",
            "|     A6000|2000|      10010100|    10|A6000| 10010100|\n",
            "|      H100|9000|      10010500|    11| H100| 10010500|\n",
            "+----------+----+--------------+------+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============= EDIT HERE =============\n",
        "DF_join.createOrReplaceTempView(\"join\")\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM join\n",
        "WHERE view > 1500\n",
        "\"\"\"\n",
        "\n",
        "# =====================================\n",
        "sqlDF = spark.sql(query)\n",
        "sqlDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRHedpkuP2jH"
      },
      "source": [
        "#### 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_EM7O2qP2jH"
      },
      "source": [
        "#### (a) Load the provided dataset, convert it into a DataFrame, and show it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnbVxy-kP2jH",
        "outputId": "13175f15-dc8d-4469-e546-9d3137884424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|  [12.0,1.45,3.6]|  2.0|\n",
            "|  [12.72,2.2,3.9]|  2.0|\n",
            "| [12.08,2.56,2.9]|  2.0|\n",
            "|  [14.1,2.75,6.2]|  1.0|\n",
            "| [13.74,2.6,5.85]|  1.0|\n",
            "|[12.37,1.98,1.95]|  2.0|\n",
            "|[13.73,1.28,6.62]|  3.0|\n",
            "|  [14.22,3.0,5.1]|  1.0|\n",
            "| [14.22,3.2,6.38]|  1.0|\n",
            "| [13.05,3.0,5.04]|  1.0|\n",
            "|  [12.33,1.9,3.4]|  2.0|\n",
            "| [13.76,2.95,5.4]|  1.0|\n",
            "|  [14.19,3.3,8.7]|  1.0|\n",
            "|  [13.72,3.4,6.8]|  1.0|\n",
            "| [11.79,2.13,3.0]|  2.0|\n",
            "| [14.16,1.68,9.7]|  3.0|\n",
            "|[13.32,1.93,8.42]|  3.0|\n",
            "|  [13.75,2.6,5.6]|  1.0|\n",
            "|  [12.08,1.6,2.4]|  2.0|\n",
            "|  [12.47,2.5,2.6]|  2.0|\n",
            "+-----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|[13.69,1.83,5.88]|  3.0|\n",
            "| [12.42,2.0,2.06]|  2.0|\n",
            "|  [13.64,2.7,5.1]|  1.0|\n",
            "|[12.21,1.85,2.85]|  2.0|\n",
            "|  [13.77,3.0,6.3]|  1.0|\n",
            "| [13.49,1.62,5.7]|  3.0|\n",
            "| [11.76,1.75,3.8]|  2.0|\n",
            "|  [14.38,3.3,7.5]|  1.0|\n",
            "| [12.36,2.3,7.65]|  3.0|\n",
            "| [12.72,1.38,3.3]|  2.0|\n",
            "|  [14.12,2.2,5.0]|  1.0|\n",
            "| [13.24,2.8,4.32]|  1.0|\n",
            "| [12.22,2.36,2.7]|  2.0|\n",
            "|[13.88,3.25,5.43]|  1.0|\n",
            "| [11.84,2.2,3.05]|  2.0|\n",
            "|[11.41,2.48,3.08]|  2.0|\n",
            "|  [13.11,2.2,7.1]|  3.0|\n",
            "|  [13.48,2.7,5.1]|  1.0|\n",
            "| [12.42,1.68,2.7]|  2.0|\n",
            "| [13.58,2.86,6.9]|  1.0|\n",
            "+-----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ClassificationPractice\").getOrCreate()\n",
        "\n",
        "train_df = spark.read.csv(\"./dataset/train_wine.csv\", header=True, inferSchema=True)\n",
        "test_df = spark.read.csv(\"./dataset/test_wine.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "columns = train_df.columns[1:]\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "\n",
        "# Instruction 1: Assemble the features into a vector column and name the column \"features\"\n",
        "# Instruction 2: Rename the target column to \"label\"\n",
        "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
        "train_df = assembler.transform(train_df)\n",
        "train_df = train_df.withColumnRenamed(\"Type\", \"label\")\n",
        "train_df = train_df.select(\"features\", \"label\")\n",
        "\n",
        "test_df = assembler.transform(test_df)\n",
        "test_df = test_df.withColumnRenamed(\"Type\", \"label\")\n",
        "test_df = test_df.select(\"features\", \"label\")\n",
        "\n",
        "train_df.show()\n",
        "test_df.show()\n",
        "\n",
        "# ====================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg3LydRYP2jH"
      },
      "source": [
        "#### (b) Use K-means clustering to classify the data. Then, write the prediction count for each wine type with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2laCia3P2jH",
        "outputId": "5857089e-619b-4d88-900e-325b6812f6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+-----+\n",
            "|Type|prediction|count|\n",
            "+----+----------+-----+\n",
            "| 2.0|         1|   13|\n",
            "| 1.0|         0|    7|\n",
            "| 3.0|         1|    4|\n",
            "| 1.0|         1|    7|\n",
            "| 3.0|         0|    5|\n",
            "+----+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 3-(b)\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "kmeans = KMeans(\n",
        "\t\t \t\tfeaturesCol='features',\n",
        "\t\t\t\tpredictionCol='prediction',\n",
        "\t\t\t\tk=2,\n",
        "\t\t\t\tmaxIter=20,\n",
        "\t\t\t\tdistanceMeasure='euclidean')\n",
        "model = kmeans.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "selected_columns = predictions.select(\"label\", \"prediction\")\n",
        "predictions = predictions.withColumnRenamed(\"label\",\"Type\")\n",
        "output_groupby_columns = predictions.groupBy('Type', 'prediction')\n",
        "\n",
        "# =====================================\n",
        "\n",
        "output_groupby_columns.count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKWNzZotP2jI"
      },
      "source": [
        "#### (c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zViDIyvPP2jI"
      },
      "outputs": [],
      "source": [
        "# Multi-class classification evaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIb-EF9-P2jI"
      },
      "source": [
        "### Logistic Regression\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6mJH07zP2jI",
        "outputId": "0a3f2d8a-2ea7-4e9d-88d3-3e799c2f704c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Training and Test\n",
        "# ============= EDIT HERE =============\n",
        "lr_model = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
        "lr_preds = lr_model.fit(train_df).transform(test_df)\n",
        "lr_accuracy = evaluator.evaluate(lr_preds)\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {lr_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awdqz-lDP2jJ"
      },
      "source": [
        "### Decision Tree\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1vsoY1sP2jJ",
        "outputId": "e062df50-ae4b-4047-ef02-b87332a81529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9166666666666666\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# On model declaration, fix seed, maxDepth,  to the following values\n",
        "seed = 2024\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "\n",
        "dt_model = DecisionTreeClassifier(featuresCol='features', labelCol='label', seed=seed)\n",
        "dt_preds = dt_model.fit(train_df).transform(test_df)\n",
        "dt_accuracy = evaluator.evaluate(dt_preds)\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {dt_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNm2Lo5NP2jJ"
      },
      "source": [
        "### SVM\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LinearSVC.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kelkERLP2jJ",
        "outputId": "bf655a7e-efd0-4394-a260-5570fd4718e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "# On model declaration, fix seed to the following values\n",
        "seed = 2024\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "filtered_train_df = train_df.filter((train_df['label'] == 1) | (train_df['label'] == 2)).withColumn('label', when(train_df['label'] == 1, 0).otherwise(1))\n",
        "filtered_test_df = test_df.filter((test_df['label'] == 1) | (test_df['label'] == 2)).withColumn('label', when(test_df['label'] == 1, 0).otherwise(1))\n",
        "\n",
        "svm_model = LinearSVC(featuresCol='features', labelCol='label')\n",
        "svm_preds = svm_model.fit(filtered_train_df).transform(filtered_test_df)\n",
        "rfc_accuracy = evaluator.evaluate(svm_preds)\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {rfc_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFZv0lrxP2jJ"
      },
      "source": [
        "# Make sure to save both the code and output values prior to submission."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "minch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}